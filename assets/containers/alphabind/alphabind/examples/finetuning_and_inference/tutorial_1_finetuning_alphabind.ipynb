{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning AlphaBind to your data and running inference\n",
    "**2024-10-21**\n",
    "\n",
    "This tutorial shows a user how to fine-tune Alphabind to their own specific PPI data. We will cover the following steps:\n",
    "\n",
    "1. Preparing your data to be consumed by AlphaBind\n",
    "2. Fine-tuning Alphabind to customize it to your protein-protein interaction (PPI) data\n",
    "3. Running inference to predict on a new dataset\n",
    "\n",
    "## 1) Prepare your data for AlphaBind\n",
    "\n",
    "In order to fine-tune the AlphaBind pre-trained model checkpoint, you will need to convert your dataset into something that the model's inputs expect. AlphaBind predictions are based on the AlphaSeq assay. You must always format your data in the \"YM\" or \"YeastMating\" format. This requires 3 critical columns in your dataset:\n",
    "- `sequence_a`: The peptide sequence of one of the proteins in a PPI\n",
    "- `sequence_alpha`: The peptide sequence of the second protein in a PPI\n",
    "- `Kd`: A measurement or proxy measurement for binding affinity. NOTE: The AlphaBind pre-trained model was trained on data expressed as log10-Kd. Your data does not necessarily need to be in the same range or scale, however we strongly recommend that it should follow the same sign convention (i.e. large negative values represent strong binders, and large positive values represent weak binders).\n",
    "\n",
    "The above columns are case-sensitive and mandatory. While you do not need explicit AlphaSeq assay data to use AlphaBind, you need to derive the analogs to each of these columns in your native dataset.\n",
    "\n",
    "### NOTE: `sequence_a` and `sequence_alpha` assignment convention\n",
    "\n",
    "When assigning sequences, we recommend choosing a consistent convention for assigning functional classes of proteins to `sequence_a` vs. `sequence_alpha`. For example, in a dataset of antibody-antigen interactions, provide sequences for all antibodies and their variants in `sequence_a` only, and provide all antigens and their respective variants in `sequence_alpha` only.\n",
    "\n",
    "**NOTE**: If you intend to run optimization to design your own sequences later, ensure the protein that you would want modifications to is `sequence_a`. For example, if you want to generate diverse antibodies against a known target, your `sequence_a` would be the antibodies and your `sequence_alpha` would be the target antigen sequence.\n",
    "\n",
    "### NOTE: Max sequence length\n",
    "\n",
    "The AlphaBind model pre-trained checkpoint supports a max combined sequence length (`sequence_a` and `sequence_alpha`) of 600 amino acids (AA). Ensure that the combined length of each PPI pair in your dataset is within this range.\n",
    "\n",
    "### Example: Pre-processing the data from _Mason et al._\n",
    "\n",
    "For an illustrative example, We will use an example from [Mason et al](https://www.nature.com/articles/s41551-021-00699-9). Here, we will transform the data into something that looks YM-like, even though the original experimental assay is not AlphaSeq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Basic EDA on the Dataset\n",
    "\n",
    "The authors of the paper generated 3 libraries of Trastuzumab variants in the IgG format with mutations in the CDRH3 region. The starting sequence was a non-binding variant. Libraries were then transfected into mammalian cells and transfected into FACS. One round of enrichment occurred, to identify an Antigen binding (Ag+) and non-binding (Ag-) population. Another round of enrichment on the Ag+ population allowed further refinement of stronger binders (Ag+2) population. Thus, three binding populations are defined: Ag+2, Ag+, and Ag-.\n",
    "\n",
    "The most relevant columns of the dataset are as follows:\n",
    "- `AASeq`: CDRH3 sequence\n",
    "- `Enrichment Ratio`: Proxy for affinity\n",
    "- `VH Seq`: Full antibody sequence\n",
    "- `Target Seq`: Target antigen sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"alphabind/examples/data/raw/DSM_enrichment_full_seqs.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Cleaning and preparing the data\n",
    "\n",
    "In the dataset, we want to ensure the following:\n",
    "- There is a uniquely assigned score to each PPI (i.e. exactly one `Kd` value for each unique `sequence_a` : `sequence_alpha` pair)\n",
    "- [_Training Data Only_]: There are no missing or `NaN` values for the `Kd` score.\n",
    "\n",
    "We check the following:\n",
    "- Number of unique antibody sequences/CDHR3 sequences\n",
    "- Number of unique antigens\n",
    "- Number of NaN counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique CDHR3 sequences\", df[\"AASeq\"].nunique(), \"/\", df.shape[0])\n",
    "print(\"Unique VH sequences\", df[\"VH Seq\"].nunique(), \"/\", df.shape[0])\n",
    "print(\"Unique antigens\", df[\"Target Seq\"].nunique(), \"/\", df.shape[0])\n",
    "print(\"Missing affinities?\", df[\"Enrichment Ratio\"].isna().sum(), \"/\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the proxy for affinity, `Enrichment Ratio`, by looking at the distribution of weak and strong binders. You will see a bi-modal like distribution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "sns.histplot(data=df, x=\"Enrichment Ratio\", bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Preparing your affinity-proxy data.\n",
    "\n",
    "In the above example, the `Enrichment Ratio` is our proxy for affinity. Alphaseq data is typically in log 10 $K_d$, with a range between [-3, 3] on average. While unnecessary, **standard scaling the data** tends to be helpful (although effective ML models can learn simple linear transformations).\n",
    "\n",
    "Because of the definition of $K_d$, the stronger the binder, the lower the $K_d$. Our optimization algorithms determine \"better\" sequences by going downhill in affinity; **this means if you are intending to use any of our built-in optimization methods, you will need to ensure the sign of your affinity proxy points to the desired direction trending negative**. In this dataset, the higher the Enrichment Ratio, the \"better\", thus we will perform the following operations to make the data more amenable to AlphaBind.\n",
    "\n",
    "- (a) Standard scale the data\n",
    "- (b) Flip the sign of the ratio for further optimization\n",
    "- (c) Re-center at 2\n",
    "\n",
    "Steps (a) and (c) are optional; step (b) is mandatory IF you choose to use our optimization algorithms.\n",
    "We save this new data as `y` in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kd_center = 2.0\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[\"Kd_transformed\"] = scaler.fit_transform(df[[\"Enrichment Ratio\"]])[:, 0]\n",
    "df[\"Kd_transformed\"] = -df[\"Kd_transformed\"]\n",
    "df[\"Kd_transformed\"] = df[\"Kd_transformed\"] + Kd_center\n",
    "\n",
    "# Plot the new transform\n",
    "f, ax = plt.subplots(1, 1)\n",
    "sns.histplot(data=df, x=\"Kd_transformed\", bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Convert to a YM-format\n",
    "\n",
    "We now want to convert this dataset into something that can be used for training. We will pull out the necessary columns.\n",
    "\n",
    "We assign `VH Seq` to `sequence_a`, `Target Seq` to `sequence_alpha`, and `Enrichment Ratio` to `Kd`. Please note, the column names are case sensitive AND required for fine tuning. For simplicity, you may also include other columns like `description_a` or `description_alpha` to keep track of what these sequences are. Only the three aforementioned columns are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymdf = pd.DataFrame()\n",
    "ymdf[\"sequence_a\"] = df[\"VH Seq\"]\n",
    "ymdf[\"sequence_alpha\"] = df[\"Target Seq\"]\n",
    "ymdf[\"Kd\"] = df[\"Kd_transformed\"]\n",
    "ymdf.to_csv(\n",
    "    \"alphabind/examples/data/preprocessed/train_data.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Train and predict on your dataset\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "#### Data Pre-processing\n",
    "\n",
    "This tutorial assumes the user has already prepared their data per section `1)` above. At minimum, you will need to ensure that your dataset has a `sequence_a`, `sequence_alpha`, and `Kd` column for training. \n",
    "\n",
    "#### Build the `alphabind` Docker image\n",
    "\n",
    "The following steps depend on the `alphabind` Docker image, which you will need to build yourself due to BioNeMo's licensing restrictions. See the primary [`README.md`](../../../README.md) for detailed instructions of how to build that image.\n",
    "\n",
    "### A. Configuring `.env` files\n",
    "\n",
    "Sample config files for each respective step are provided in `alphabind/examples/finetuning_and_inference/conf`\n",
    "\n",
    "These files specify environment variables which are read by Click to provision function arguments for each step. These environment variables are all prefixed with `ALPHABIND_`, but otherwise correspond exactly to the argument names defined in the respective module for each step.\n",
    "\n",
    "For example, in `embed.env`, the `ALPHABIND_INPUT_FILEPATH` environment variable corresponds to the `input_filepath` argument of the `alphabind.features.build_features` module. For more details on each argument, consult the provided docstring in the source code. For convenience, we also provide a brief overview of the arguments here:\n",
    "\n",
    "#### `embed.env`\n",
    "\n",
    "- `ALPHABIND_INPUT_FILEPATH`: Path of your training CSV in the Docker guest. For this tutorial, this is `/mnt/data/preprocessed/train_data.csv`\n",
    "- `ALPHABIND_OUTPUT_FILEPATH`: Destination path for the featurized CSV created from `ALPHABIND_INPUT_FILEPATH`, which includes the pre-computed ESM embeddings. For this tutorial, this is `/mnt/data/embeddings/train_data_featurized.csv`\n",
    "- `ALPHABIND_EMBEDDING_DIR_PATH`: Destination directory path for the raw embeddings. e.g. `/mnt/data/embeddings/raw`\n",
    "- `ALPHABIND_BATCH_SIZE`: Batch size to use for computing embeddings. Default is `16` that works well on `p3.2xlarge` instances.\n",
    "\n",
    "#### `train.env`\n",
    "\n",
    "- `ALPHABIND_DATASET_CSV_PATH`: Path inside the Docker guest to the pre-featurized training data. In this tutorial that is `/mnt/data/embeddings/train_data_featurized.csv`.\n",
    "- `ALPHABIND_TX_MODEL_PATH`: Path inside the Docker guest to the AlphaBind model pre-trained checkpoint, `/mnt/models/alphabind_pretrained_checkpoint.pt`.\n",
    "- `ALPHABIND_MAX_EPOCHS`: Number of epochs to train for. Note that the current code does not use an `EarlyStopping` callback, the model will always be trained for this many epochs. However, we only return the single best model by `val_loss`, so the returned model may be from an earlier epoch.\n",
    "- `ALPHABIND_OUTPUT_MODEL_PATH`: Destination path in the Docker guest for the final best model by `val_loss`, e.g. `/mnt/data/models/model_trained.pt`. **CAUTION**: This path will be overwritten without warning if it already exists.\n",
    "\n",
    "#### `predict.env`\n",
    "\n",
    "- `ALPHABIND_PPI_DATASET_PATH`: Path in the Docker guest to the data to predict on. This data does *not* need to be pre-featurized, since the script will perform that step automatically. e.g. `alphabind/examples/data/preprocessed/train_data.csv`\n",
    "- `ALPHABIND_TRAINED_MODEL_PATH`: Path in the Docker guest to the model to use for prediction, e.g. `/mnt/data/models/model_trained.pt`.\n",
    "- `ALPHABIND_OUTPUT_DATASET_PATH`: Destination path in the Docker guest for labeled predictions, e.g. `/mnt/data/embeddings/train_data_predicted.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Example workflow for fine-tuning a model on your data and using it to predict binding affinity\n",
    "\n",
    "This workflow consists of 3 steps:\n",
    "    - Pre-featurizing (embedding)\n",
    "    - Model Training\n",
    "    - Model Inference (prediction)\n",
    "\n",
    "#### Embedding Step (for ESM model types only)\n",
    "\n",
    "You will first pre-compute embeddings. This will create pre-computed ESM-2nv embeddings for your sequences, which will be used to accelerate model training in the next step. Depending on how many sequences you have, this can be a long step. The provided example `train_data.csv` completes in roughly 20 minutes on a `g5.4xlarge` instance.\n",
    "\n",
    "You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! docker run --rm -it --init --entrypoint python --gpus=all --shm-size=64G --env-file=alphabind/examples/finetuning_and_inference/conf/embed.env --name=alphabind_embed -v ./alphabind/examples/data:/mnt/data alphabind:latest -m alphabind.features.build_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Fine-tune your model\n",
    "\n",
    "**NOTE:** This is frequently a long step (~2 hours on a `g5.4xlarge` instance). We provide an inline implementation in this notebook for convenience, but you may wish to manually run this step in a terminal emulator (e.g. `tmux`, `screen`) in order to increase resilience of the training job against e.g. temporary network interruptions to a remote instance.\n",
    "\n",
    "Here, we illustrate fine-tuning the AlphaBind pre-trained checkpoint on novel experimental data. You can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! docker run --rm -it --init --entrypoint python --gpus=all --shm-size=64G --env-file=alphabind/examples/finetuning_and_inference/conf/train.env --name=alphabind_train -v ./alphabind/examples/data:/mnt/data -v ./alphabind/models:/mnt/models alphabind:latest -m alphabind.models.train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking model performance\n",
    "\n",
    "Your most-recent training metrics should have been dumped to (relative to the repo root) `./alphabind/examples/data/models/logs/lightning_logs/version_0/metrics.csv` (or more generally, `version_N`, where `N` is the highest present version index). You can monitor this output file dynamically with `tail -f metrics.csv`. There are a few quantities you may want to consider:\n",
    "\n",
    "- `train_loss`: Mini batch train loss\n",
    "- `val_loss`: Validation loss per epoch\n",
    "- `spearman_rho`: Validation Spearman Rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using your trained model\n",
    "\n",
    "If you have a separate file to run inference on, you can run that as follows (here we simply predict on the original train set for illustration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! docker run --rm -it --init --entrypoint python --gpus=all --shm-size=64G --env-file=alphabind/examples/finetuning_and_inference/conf/predict.env --name=alphabind_predict -v ./alphabind/examples/data:/mnt/data -v ./alphabind/models:/mnt/models alphabind:latest -m alphabind.models.predict_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare your prediction data similarly to your training data. You do not need to include a `Kd` feature; it is sufficient to have only properly setup `sequence_a` and `sequence_alpha` columns. Prediction dynamically runs BioNeMo under the hood, meaning embeddings are re-computed for each PPI pair of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Mason, D.M., Friedensohn, S., Weber, C.R. et al. Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning. Nat Biomed Eng 5, 600â€“612 (2021). https://doi.org/10.1038/s41551-021-00699-9 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
